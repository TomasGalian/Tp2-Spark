{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Para las películas que tengan en su descripción “in a world”(sin importar las mayúsculas) cuales son las 10 tuplas de palabras en sus descripciones más populares?"
      ],
      "metadata": {
        "id": "w8IbWfsT_5k1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxKiBw9D-OLV",
        "outputId": "8d3254a6-77c6-4963-822f-b0e58a2587d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 64 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 82.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=cae3bfdc201f589916f3533f706a2a7cc0b964459be752408e143ec7c650310a\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ],
      "source": [
        "#Realizamos las importanciones\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Realizamos las importaciones para usar google drive y pyspark\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "YHU6_MhL-phX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "DtAZO599-qXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Descargamos movies.parquet\n",
        "descarga = drive.CreateFile({'id':'1YKh46_KdgSC-6UZ0mttbOqVI6tXgly-F'})\n",
        "descarga.GetContentFile('movies.parquet')"
      ],
      "metadata": {
        "id": "RmVXC0LQ-rLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.getOrCreate() #Obtenemos una sesion de spark\n",
        "sc = spark.sparkContext #Obtenemos el contexto"
      ],
      "metadata": {
        "id": "SS8AgRpK-yDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creo los rdd con los que voy a trabajar\n",
        "sqlContext = SQLContext(sc)\n",
        "df = sqlContext.read.parquet('movies.parquet',header=True,inferSchema=True)\n",
        "moviesRdd = df.rdd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNVDhMoF-zSd",
        "outputId": "38ec4881-2fdb-4a13-d167-79ef3d80b6dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:114: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Filtro aquellas peliculas que no tengan descripccion\n",
        "moviesRdd = moviesRdd.filter(lambda x: x.overview != None).filter(lambda x: 'in a world' in x.overview.lower())"
      ],
      "metadata": {
        "id": "RZ-llR0R_-s0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Me fijo con cuantas peliculas me estoy quedando\n",
        "moviesRdd.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ox4w0v-I_h-F",
        "outputId": "0ab50f7e-fa37-4305-d2d9-24e6294b7eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "111"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Para obtener las tuplas lo primero que hago es mapear mis registros de forma tal que me quede con las descripciones de cada pelicula. Luego las pongo en miniscula (ya que no tomo en cuentas las mayusculas) y spliteo para tener las palabras de las descripciones separadas\n",
        "#Una vez que tengo eso hago un flat map de forma tal que a partir de cada registro me cree un registro por tupla para todas las descripciones quedando de la siguiente forma (tupla,1).\n",
        "#Teniendo ya cada registro mapeado con la forma (clave,valor) donde mi clave son las tuplas hago un reduceByKey contando las veces que se repitan esas tuplas. Por ultimo lo ordeno con un takeOrdered y me quedan las tuplas\n",
        "moviesRdd.map(lambda x: x.overview.lower().split()).flatMap(lambda x: [((x[w]+','+x[w+1]),1) for w in range(len(x)-1)]).reduceByKey(lambda a,b:a+b).takeOrdered(10, key=lambda x: -x[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4Jmr5QuVteZ",
        "outputId": "b3264881-fd11-4371-909d-75cf6aa7f8e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('in,a', 131),\n",
              " ('a,world', 110),\n",
              " ('world,where', 37),\n",
              " ('of,the', 34),\n",
              " ('world,of', 26),\n",
              " ('in,the', 20),\n",
              " ('to,the', 18),\n",
              " ('of,a', 15),\n",
              " ('is,a', 15),\n",
              " ('the,story', 11)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cuando utilizo el \n",
        "\n",
        "```\n",
        "((x[w]+','+x[w+1]),1) for w in range(len(x)-1)\n",
        "```\n",
        "dentro del flatmap lo que hago es lo siguiente. En x tengo una lista de todas las palabras que tiene la descripcion de una pelicula, por lo tanto si mi descripcion tiene 5 palabras mi for ira del 0 al 4. Luego para generar mis registros lo que hago es referirme a la primera palabra con x[w] y a la siguiente con x[w+1] de forma tal que queden armadas todas las tuplas\n"
      ],
      "metadata": {
        "id": "x1gIPix3FbSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8SzNF579EsGW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}